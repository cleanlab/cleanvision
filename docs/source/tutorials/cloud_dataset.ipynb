{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CleanVision on a dataset in cloud\n",
    "\n",
    "CleanVision checks can be run on a dataset residing in a cloud storage as well. It currently supports S3, Google Cloud Storage and Azure Blob Storage. Before using CleanVision on a cloud dataset, the appropriate storage needs to be configured on the machine. See the recommended ways to configure different storages on the machine if not already configured\n",
    "\n",
    "- S3 (`\"s3://\"`): `aws configure` (same as `boto` configuration).\n",
    "- Google Storage (`\"gs://\"`): `gcloud auth login`.\n",
    "- Azure Blob Storage (`\"az://\"`): you need to pass your secrets in the `storage_opts`. This is also an option for S3 and Google Storage but is generally not recommended.\n",
    "\n",
    "Internally CleanVision uses `fsspec` library to support the above cloud datasets. Check the [fsspec documentation](https://filesystem-spec.readthedocs.io/en/latest/) to learn more.\n",
    "\n",
    "If you are in the cloud the secrets for dataset access may already exist in your environment. In this case, you may not need to do anything. \n",
    "\n",
    "### Install optional dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install \"cleanvision[s3] @ git+https://github.com/cleanlab/cleanvision.git\" # for aws\n",
    "# !pip install \"cleanvision[azure] @ git+https://github.com/cleanlab/cleanvision.git\" for azure blob storage\n",
    "# !pip install \"cleanvision[gcs] @ git+https://github.com/cleanlab/cleanvision.git\" for google cloud storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After you install these packages, you may need to restart your notebook runtime before running the rest of this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanvision.imagelab import Imagelab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dataset path and optional arguments\n",
    "\n",
    "For running CleanVision, we use [Amazon Berkeley Objects (ABO)](https://amazon-berkeley-objects.s3.amazonaws.com/index.html#) dataset that is publicly available on [S3](https://amazon-berkeley-objects.s3.amazonaws.com/index.html#aws). Here we are accessing the dataset using an anonymous connection, however for a private dataset you can supply the credentials using config or environment variables and set `anon=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_path = \"s3://amazon-berkeley-objects/images/small/aa/\"\n",
    "storage_opts = {\"anon\": True}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CleanVision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagelab = Imagelab(data_path=cloud_path, storage_opts=storage_opts)\n",
    "imagelab.find_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagelab.report()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicitly authenticating with credentials\n",
    "Authentication can also be done by passing appropriate environments to the optional argument `storage_opts`. \n",
    "\n",
    "We install a dotenv environment for better handling of the secrets.\n",
    "\n",
    "Create a `.env` file in the root directory with the following contents:\n",
    "\n",
    "```\n",
    "AZURE_STORAGE_ACCOUNT_NAME=XXXXX # your account name\n",
    "AZURE_STORAGE_ACCOUNT_KEY=XXXX # storage account key to your storage account\n",
    "```\n",
    "\n",
    "It's also possible to pass the S3 credentials in the same way:\n",
    "```\n",
    "AWS_ACCESS_KEY_ID=XXXX\n",
    "AWS_SECRET_ACCESS_KEY=XXX\n",
    "```\n",
    "\n",
    "Install python-dotenv\n",
    "\n",
    "```shell\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "The following code snippet shows how to supply credentials explicitly for azure storage.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# the container is called `main` and it contains a folder called `test-dataset`\n",
    "cloud_path = \"az://main/test-dataset/\"\n",
    "ACCOUNT_KEY = os.environ.get(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "ACCOUNT_NAME = os.environ.get(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "storage_opts = {\"account_name\": ACCOUNT_NAME, \"account_key\": ACCOUNT_KEY}\n",
    "imagelab = Imagelab(data_path=cloud_path, storage_opts=storage_opts)\n",
    "imagelab.find_issues()\n",
    "imagelab.report()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cleanvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
