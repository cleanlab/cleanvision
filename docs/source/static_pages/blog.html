<!DOCTYPE html>

<!-- saved from url=(0043)https://cleanlab.ai/blog/learn/cleanvision/ -->
<html data-theme="light" lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><meta content="width=device-width" data-next-head="" name="viewport"/><script id="hs-analytics" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/39575311.js" type="text/javascript"></script><script crossorigin="anonymous" data-hsjs-env="prod" data-hsjs-hublet="na2" data-hsjs-portal="39575311" data-leadin-env="prod" data-leadin-portal-id="39575311" data-loader="hs-scriptloader" id="CollectedForms-39575311" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/collectedforms.js" type="text/javascript"></script><script data-cookieconsent="ignore" data-hs-ignore="true" data-hsjs-env="prod" data-hsjs-hublet="na2" data-hsjs-portal="39575311" data-loader="hs-scriptloader" id="cookieBanner-39575311" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/banner.js" type="text/javascript"></script><script crossorigin="anonymous" data-hsjs-env="prod" data-hsjs-hublet="na2" data-hsjs-portal="39575311" data-loader="hs-scriptloader" id="hubspot-web-interactives-loader" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/web-interactives-embed.js" type="text/javascript"></script><script async="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/insight.min.js" type="text/javascript"></script><script async="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/array.js" type="text/javascript"></script><script async="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/reb2b.js.gz" type="text/javascript"></script><script async="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/js"></script><script async="" defer="" id="hs-script-loader" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/39575311(1).js" type="text/javascript"></script><script async="" defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/buttons.js"></script><title data-next-head="">CleanVision: Audit your Image Data for better Computer Vision</title><link data-next-head="" href="https://cleanlab.ai/favicon.svg" rel="icon" type="image/svg+xml"/><link data-next-head="" href="https://cleanlab.ai/favicon.ico" rel="alternate icon"/><meta content="CleanVision: Audit your Image Data for better Computer Vision" data-next-head="" property="og:title"/><meta content="CleanVision: Audit your Image Data for better Computer Vision" data-next-head="" name="twitter:title"/><meta content="Cleanlab" data-next-head="" property="og:site_name"/><meta content="@CleanlabAI" data-next-head="" name="twitter:site"/><meta content="https://cleanlab.ai/blog/learn/cleanvision/" data-next-head="" property="og:url"/><meta content="Introducing an open-source Python package to automatically identify common issues in image datasets." data-next-head="" name="description"/><meta content="Introducing an open-source Python package to automatically identify common issues in image datasets." data-next-head="" property="og:description"/><meta content="Introducing an open-source Python package to automatically identify common issues in image datasets." data-next-head="" name="twitter:description"/><meta content="XX2s5qlf2L8JLlr_vp8asuq-YtPtEwO8qU-b3FtkkP8" data-next-head="" name="google-site-verification"/><meta content="https://cleanlab.ai/_next/static/media/thumb.9b34cb17.png" data-next-head="" property="og:image"/><meta content="summary_large_image" data-next-head="" name="twitter:card"/><meta content="https://cleanlab.ai/_next/static/media/thumb.9b34cb17.png" data-next-head="" name="twitter:image"/><link href="https://cleanlab.ai/favicon.ico" rel="icon" sizes="16x16 32x32"/><link href="https://cleanlab.ai/favicon.svg" rel="icon" type="image/svg+xml"/><link href="https://cleanlab.ai/apple-touch-icon.png" rel="apple-touch-icon"/><link href="https://cleanlab.ai/site.webmanifest" rel="manifest"/><link href="https://fonts.googleapis.com/" rel="preconnect"/><link crossorigin="anonymous" href="https://fonts.gstatic.com/" rel="preconnect"/><link crossorigin="anonymous" data-next-font="size-adjust" href="https://cleanlab.ai/" rel="preconnect"/><link as="style" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/888b90cb8567afac.css" rel="preload"/><link as="style" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/a884c8f6bdf23ae1.css" rel="preload"/><link href="./CleanVision_ Audit your Image Data for better Computer Vision_files/css2" rel="stylesheet"/><link href="./CleanVision_ Audit your Image Data for better Computer Vision_files/css2(1)" rel="stylesheet"/><link href="./CleanVision_ Audit your Image Data for better Computer Vision_files/css2(2)" rel="stylesheet"/><link href="./CleanVision_ Audit your Image Data for better Computer Vision_files/css2(3)" rel="stylesheet"/><script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){dataLayer.push(arguments);}
                gtag('js', new Date());
                gtag('config', 'G-5P3P0GBS4F', {
                  page_path: window.location.pathname,
                });</script><script>!function () {var reb2b = window.reb2b = window.reb2b || [];if (reb2b.invoked) return;reb2b.invoked = true;reb2b.methods = ["identify", "collect"];reb2b.factory = function (method) {return function () {var args = Array.prototype.slice.call(arguments);args.unshift(method);reb2b.push(args);return reb2b;};};for (var i = 0; i < reb2b.methods.length; i++) {var key = reb2b.methods[i];reb2b[key] = reb2b.factory(key);}reb2b.load = function (key) {var script = document.createElement("script");script.type = "text/javascript";script.async = true;script.src = "https://s3-us-west-2.amazonaws.com/b2bjsstore/b/" + key + "/reb2b.js.gz";var first = document.getElementsByTagName("script")[0];first.parentNode.insertBefore(script, first);};reb2b.SNIPPET_VERSION = "1.0.1";reb2b.load("YE63P0HD15OW");}();</script><script>
                window[(function(_H1f,_Zk){var _MZjh6='';for(var _bPdXnf=0;_bPdXnf<_H1f.length;_bPdXnf++){var _gIPm=_H1f[_bPdXnf].charCodeAt();_gIPm-=_Zk;_MZjh6==_MZjh6;_gIPm+=61;_Zk>3;_gIPm%=94;_gIPm+=33;_gIPm!=_bPdXnf;_MZjh6+=String.fromCharCode(_gIPm)}return _MZjh6})(atob('KXZ9QT45NDJDeDRI'), 45)] = '234bf5ed6d1712010871'; var zi = document.createElement('script'); (zi.type = 'text/javascript'), (zi.async = true), (zi.src = (function(_bmP,_mT){var _l4BNN='';for(var _0jK6MB=0;_0jK6MB<_bmP.length;_0jK6MB++){var _dEvA=_bmP[_0jK6MB].charCodeAt();_dEvA!=_0jK6MB;_dEvA-=_mT;_l4BNN==_l4BNN;_dEvA+=61;_mT>2;_dEvA%=94;_dEvA+=33;_l4BNN+=String.fromCharCode(_dEvA)}return _l4BNN})(atob('aXV1cXQ7MDBrdC97ai50ZHNqcXV0L2RwbjB7ai51Ymgva3Q='), 1)), document.readyState === 'complete'?document.body.appendChild(zi): window.addEventListener('load', function(){ document.body.appendChild(zi) });
            </script><script>
                !function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.async=!0,p.src=s.api_host+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="capture identify alias people.set people.set_once set_config register register_once unregister opt_out_capturing has_opted_out_capturing opt_in_capturing reset isFeatureEnabled onFeatureFlags getFeatureFlag getFeatureFlagPayload reloadFeatureFlags group updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures getActiveMatchingSurveys getSurveys onSessionId".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]); posthog.init('phc_IqHg4uTP2wPykNG21gDwDBqarxhTswDKPj9cIUxIuoe',{api_host:'https://app.posthog.com'})
              </script><noscript><style>.hide-without-js {
                display: none;
              }</style></noscript><style> @font-face {
                  font-display: block;
                  font-family: Roboto;
                  src: url(https://assets.brevo.com/font/Roboto/Latin/normal/normal/7529907e9eaf8ebb5220c5f9850e3811.woff2)
                      format(woff2),
                    url(https://assets.brevo.com/font/Roboto/Latin/normal/normal/25c678feafdc175a70922a116c9be3e7.woff)
                      format(woff);
                }
                @font-face {
                  font-display: fallback;
                  font-family: Roboto;
                  font-weight: 600;
                  src: url(https://assets.brevo.com/font/Roboto/Latin/medium/normal/6e9caeeafb1f3491be3e32744bc30440.woff2)
                      format(woff2),
                    url(https://assets.brevo.com/font/Roboto/Latin/medium/normal/71501f0d8d5aa95960f6475d5487d4c2.woff)
                      format(woff);
                }
                @font-face {
                  font-display: fallback;
                  font-family: Roboto;
                  font-weight: 700;
                  src: url(https://assets.brevo.com/font/Roboto/Latin/bold/normal/3ef7cf158f310cf752d5ad08cd0e7e60.woff2)
                      format(woff2),
                    url(https://assets.brevo.com/font/Roboto/Latin/bold/normal/ece3a1d82f18b60bcce0211725c476aa.woff)
                      format(woff);
                }
                #sib-container input:-ms-input-placeholder {
                  text-align: left;
                  font-family: Helvetica, sans-serif;
                  color: #c0ccda;
                }
                #sib-container input::placeholder {
                  text-align: left;
                  font-family: Helvetica, sans-serif;
                  color: #c0ccda;
                }
                #sib-container textarea::placeholder {
                  text-align: left;
                  font-family: Helvetica, sans-serif;
                  color: #c0ccda;
            }</style><link href="./CleanVision_ Audit your Image Data for better Computer Vision_files/sib-styles.css" rel="stylesheet"/><link data-n-g="" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/888b90cb8567afac.css" rel="stylesheet"/><link data-n-p="" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/a884c8f6bdf23ae1.css" rel="stylesheet"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/polyfills-42372ed130431b0a.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/webpack-1f61b32aceac5130.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/framework-d7945a8ad0653f37.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/main-3cda69273e304fc0.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/_app-7316b31da839f590.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/910-8ed4e11e51fe6000.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/4587-e827c4b40ee0777e.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/9840-a8af020c4da657ca.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/966-8ddea9a20e331943.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/502-fd8e07d59a0eae4f.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/3080-cdfc06590dfdfced.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/2705-d8cf08995015fdf4.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/1316-2ca6f900bb516a4c.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/5428-bbb8e65029047272.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/[slug]-b62e78795290f453.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/_buildManifest.js"></script><script defer="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/_ssgManifest.js"></script><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/8bdd1454-90018e4d60423a3c.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/5388-f1a9e1e47a423be7.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/9434-34c412f0c619f7d7.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/index-76410b38ed8a1cb4.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/contact-497081860c994020.js" rel="prefetch"/><style id="_goober"> .go2933276541{position:fixed;display:block;width:100%;height:0px;margin:0px;padding:0px;overflow:visible;transform-style:preserve-3d;background:transparent;backface-visibility:hidden;pointer-events:none;left:0px;z-index:9998;}.go2369186930{top:0px;z-index:9999;height:100%;width:100%;}.go1348078617{bottom:0px;}.go2417249464{position:fixed;z-index:9989;}.go3921366393{left:0;bottom:0;}.go3967842156{right:0;bottom:0;}.go613305155{left:0;top:0;}.go471583506{right:0;top:0;}.go3670563033{position:relative;overflow:hidden;display:none;}.go1041095097{display:block;}.go1632949049{position:absolute;pointer-events:none;width:101vw;height:101vh;background:rgba(0,0,0,0.7);opacity:0;z-index:-1;}.go2512015367{z-index:99998;opacity:0.8;visibility:visible;pointer-events:all;cursor:pointer;}.go1432718904{overflow:hidden;}.go812842568{display:block !important;position:static !important;box-sizing:border-box !important;background:transparent !important;border:none;min-height:0px !important;max-height:none !important;margin:0px;padding:0px !important;height:100% !important;width:1px !important;max-width:100% !important;min-width:100% !important;}.go3064412225{z-index:99999;visibility:hidden;position:absolute;inset:50vh auto auto 50%;left:50%;top:50%;transform:translate(-50%,-50%) translateY(100vh);pointer-events:none;max-height:95%;max-width:95%;}.go1656994552{pointer-events:auto !important;visibility:visible;transform:translate(-50%,-50%) translateY(0);transition:transform 0.75s linear(0,0.006,0.023 2.2%,0.096 4.8%,0.532 15.4%,0.72 21%,0.793,0.853 26.7%,0.902,0.941,0.968 36.2%,0.987 39.7%,1 43.7%,1.007 48.3%,1.009 55.3%,1.002 78.2%,1 );}.go456419034{transition:opacity 0.3s ease-in;}.go3128134379{pointer-events:auto !important;visibility:visible !important;max-height:95vh !important;transition:max-height 1s ease-in;}.go494047706{z-index:9999;width:100%;max-height:95%;position:fixed;visibility:hidden;}.go2481764524{z-index:9999;width:100%;max-height:95%;position:fixed;visibility:hidden;bottom:0px;}.go2685733372{visibility:hidden;}.go2985984737{visibility:visible !important;}.go3281949485{pointer-events:auto !important;visibility:visible !important;max-height:95vh !important;transform:none !important;}.go3508454897{z-index:9999;width:100%;max-height:95%;position:fixed;visibility:hidden;transition:transform 1s linear(0,0.006,0.022 2.3%,0.091 5.1%,0.18 7.6%,0.508 16.3%,0.607,0.691,0.762,0.822 28.4%,0.872,0.912 35.1%,0.944 38.9%,0.968 43%,0.985 47.6%,0.996 53.1%,1.001 58.4%,1.003 65.1%,1 );}.go988075951{z-index:9999;position:fixed;left:10px;top:10px;max-height:95vh !important;max-width:95%;visibility:hidden;}.go2699082514{z-index:9999;position:fixed;right:10px;top:10px;max-height:95vh !important;max-width:95%;visibility:hidden;}.go1595992025{z-index:9999;position:fixed;left:10px;bottom:10px;max-height:95vh !important;max-width:95%;visibility:hidden;}.go1222083472{z-index:9999;position:fixed;right:10px;bottom:10px;max-height:95vh !important;max-width:95%;visibility:hidden;}.go722322694{transition:none !important;}.go26732895{cursor:pointer;}.go2083580917{display:flex;justify-content:center;align-items:center;}</style><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/9868-1b846e08fbe45822.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/2072-139672ef54e5e835.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/1100-affd1a526667f5f8.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/blog-61f654b447a5c73a.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/[slug]-19ab5eb3b9ed3d5c.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/detect-e09a8a05b427307a.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/2979-44437107383ef639.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/community-d47d6fe736b26899.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/about-b63f0a2bfa591698.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/remediate-0228767999a856e2.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/research-172a508a1c35af2d.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/csa-9f756dad5c5e06af.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/customers-6109daa8625247a8.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/partners-c0614dd8ecfcde4e.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/talks-5d64899916c05feb.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/ai-agents-in-production-2025-c28d00b1f2b644bf.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/team-fb11b7759cbf9e1f.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/careers-0667be188945c46a.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/news-70342c49dccc8ac5.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/trust-d4d7249583768725.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/terms-56f43b4a0e4f0f52.js" rel="prefetch"/><link as="script" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/privacy-9d4d93c42c7e0611.js" rel="prefetch"/></head><body><div class="go3670563033" id="hs-web-interactives-top-push-anchor"></div><link as="image" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/cleanlab-black.6b7015b5.png" rel="preload"/><link as="image" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/cleanlab-white.0e929061.png" rel="preload"/><link as="image" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/logo-flush-black.b93799f9.png" rel="preload"/><link as="image" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/logo-flush-white.316290bb.png" rel="preload"/><link as="image" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/sanjana.582489ef.jpg" rel="preload"/><link as="image" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/ulyana.2f559407.jpg" rel="preload"/><link as="image" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/yiming.chen.c0135558.jpg" rel="preload"/><link as="image" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/elias.8b51f9a7.jpg" rel="preload"/><link as="image" href="./CleanVision_ Audit your Image Data for better Computer Vision_files/jonas.329fb307.jpg" rel="preload"/><link as="image" imagesrcset="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcaltech.add33239.png&amp;w=1200&amp;q=75 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fcaltech.add33239.png&amp;w=3840&amp;q=75 2x" rel="preload"/><main class="min-h-[39vh] px-6 pb-13 pt-0 xs:px-8 _slug__base__F1aIp"><div class="blogs"><div class="_slug__main__L1NaY" data-proofer-ignore="true"><header class="header"><h1 class="title">CleanVision: Audit your Image Data for better Computer Vision</h1><ul class="Authors_authors___fiKx"><li><img alt="Sanjana Garg" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/sanjana.582489ef.jpg"/><span>Sanjana Garg</span></li><li><img alt="Ulyana Tkachenko" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/ulyana.2f559407.jpg"/><span>Ulyana Tkachenko</span></li><li><img alt="Yiming Chen" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/yiming.chen.c0135558.jpg"/><span>Yiming Chen</span></li><li><img alt="Elías Snorrason" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/elias.8b51f9a7.jpg"/><span>Elías Snorrason</span></li><li><img alt="Jonas Mueller" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/jonas.329fb307.jpg"/><span>Jonas Mueller</span></li></ul></header><div class="content"><div class="mdx-content contents blog-post"><p>Introducing <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://github.com/cleanlab/cleanvision" rel="noopener noreferrer" target="_blank">CleanVision</a>: an open-source Python library that scans <strong class="[font-weight:600]">any image dataset</strong> for common real-world <strong class="[font-weight:600]">issues</strong> such as images which are blurry, under/over-exposed, oddly sized, or (near) duplicates of others. Here are some of the issues CleanVision automatically detected (with just 3 lines of code) in the famous <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://data.caltech.edu/records/nyy15-4j048" rel="noopener noreferrer" target="_blank">Caltech-256 dataset</a>.</p>
<div><img alt="Issues detected in the Caltech-256 dataset" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/caltech.png" style="color:transparent" width="1200"/></div>
<p>Teaching computers <em class="italic">to see</em> via massive datasets and models has produced astounding progress for image generation, classification, segmentation, and related tasks like object detection. As computer vision techniques graduate from the lab to real-world applications, <b>data quality</b> poses a major challenge. ML models can only be as good as the data they are trained on.</p>
<p>To help you improve your data, we’ve just open-sourced <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://github.com/cleanlab/cleanvision" rel="noopener noreferrer" target="_blank">CleanVision</a>, a package to quickly audit <em class="italic">any</em> image dataset for a broad range of common issues lurking in real-world data.
Instead of relying on manual inspection, which can be time-consuming and lack coverage, CleanVision provides an automated systematic approach for detecting data issues.
With just a few lines of code (that can be used to audit any image data), we ran CleanVision on some popular computer vision datasets and identified many issues in each.</p>
<h2 class="text-pretty" id="issues-in-the-caltech-256-dataset">Issues in the Caltech-256 dataset</h2>
<p>Widely used for benchmarking object recognition methods, the <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://data.caltech.edu/records/nyy15-4j048" rel="noopener noreferrer" target="_blank">Caltech-256</a> dataset consists of 30,607 real-world images (of different sizes) stemming from 257 classes.
CleanVision identified several issues in this dataset including images that are: grayscale, low information, blurry, near duplicates, or have an odd aspect ratio. All of the images in the <em class="italic">car-side</em> class are grayscale images, which can cause models to <b>spuriously associate</b> the <em class="italic">car-side</em> class with the grayscale attribute rather than focusing on the object’s features. Moreover, CleanVision detected low-information images that do not resemble real-world images like stick figures and graphs. Closer inspection reveals some of these images are mislabeled in the dataset. For example, the sigmoid like plot in the low information set of images is wrongly labeled as a <em class="italic">housefly</em>. Detecting low-information images is also important when training generative models, as these can be easily memorized by
the model and lead to model regurgitation instead of synthesizing new images.</p>
<p>More generally, it is good to know these issues exist in your data before you dive into modeling it. Especially if they are easily detected with CleanVision!</p>
<p>Getting the above results for the Caltech-256 dataset was as easy as just running this code:</p>
<div class="my-[1em] flex flex-col"><div class="type-code-100 relative w-full overflow-hidden rounded-2 border border-border-1 bg-surface-0-hover [&amp;&gt;pre]:m-0 language-python"><div class="flex w-full items-center justify-between border-b border-border-1 bg-surface-2 px-6 py-2"><span class="type-body-100 text-text-faint">python</span><div class="flex items-center space-x-1"><button aria-label="Copy code" class="relative flex shrink-0 items-center justify-center outline outline-1 -outline-offset-1 focus-visible:outline-1 bg-surface-0 text-text-primary outline-border-1 cursor-pointer focus-visible:outline-blue-700 focus-visible:ring-2 focus-visible:ring-focus rounded-1 shadow-elev-0 hover:bg-surface-0-hover active:bg-surface-0-active" data-state="closed" type="button"><span class="flex items-center justify-center size-8"><span class="block [&amp;&gt;svg]:block [&amp;&gt;svg]:align-middle [&amp;&gt;svg]:stroke-[1.8px] text-text-primary" role="img" style="width:12px;height:12px"><svg fill="none" height="12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="12" xmlns="http://www.w3.org/2000/svg"><rect height="13" rx="2" ry="2" width="13" x="9" y="9"></rect><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path></svg></span></span></button></div></div><div class="type-code-100 m-0 flex flex-nowrap overflow-auto p-0 py-8 pr-6 [scrollbar-width:thin] prism-code language-python" style="color:hsl(var(--twc-text-primary));background-color:transparent"><div aria-hidden="true" class="sticky left-0 block h-full select-none bg-surface-1 px-6 text-right text-text-faint"><div>0</div><div>1</div><div>2</div><div>3</div><div>4</div><div>5</div></div><div><pre class="token-line" style="color:hsl(var(--twc-text-primary))"><span class="token keyword" style="color:hsl(var(--twc-cyan-800))">from</span><span class="token plain"> cleanvision</span><span class="token punctuation" style="color:hsl(var(--twc-yellow-700))">.</span><span class="token plain">imagelab </span><span class="token keyword" style="color:hsl(var(--twc-cyan-800))">import</span><span class="token plain"> Imagelab</span></pre><pre class="token-line" style="color:hsl(var(--twc-text-primary))"><span class="token plain" style="display:inline-block">
</span></pre><pre class="token-line" style="color:hsl(var(--twc-text-primary))"><span class="token plain">imagelab </span><span class="token operator" style="color:hsl(var(--twc-yellow-700))">=</span><span class="token plain"> Imagelab</span><span class="token punctuation" style="color:hsl(var(--twc-yellow-700))">(</span><span class="token plain">data_path</span><span class="token operator" style="color:hsl(var(--twc-yellow-700))">=</span><span class="token string" style="color:hsl(var(--twc-green-600))">"path_to_dataset"</span><span class="token punctuation" style="color:hsl(var(--twc-yellow-700))">)</span><span class="token plain"></span></pre><pre class="token-line" style="color:hsl(var(--twc-text-primary))"><span class="token plain">imagelab</span><span class="token punctuation" style="color:hsl(var(--twc-yellow-700))">.</span><span class="token plain">find_issues</span><span class="token punctuation" style="color:hsl(var(--twc-yellow-700))">(</span><span class="token punctuation" style="color:hsl(var(--twc-yellow-700))">)</span><span class="token plain"></span></pre><pre class="token-line" style="color:hsl(var(--twc-text-primary))"><span class="token plain">imagelab</span><span class="token punctuation" style="color:hsl(var(--twc-yellow-700))">.</span><span class="token plain">report</span><span class="token punctuation" style="color:hsl(var(--twc-yellow-700))">(</span><span class="token punctuation" style="color:hsl(var(--twc-yellow-700))">)</span><span class="token plain"></span></pre><pre class="token-line" style="color:hsl(var(--twc-text-primary))"><span class="token plain" style="display:inline-block">
</span></pre></div></div></div></div>
<p>The images can simply be files in a <code class="type-code-100 -z-10 mx-[0.15em] inline rounded-1 border border-border-1 bg-surface-1 px-[0.43em] py-[0.15em] text-[0.86em] leading-[inherit] text-text-primary">path_to_dataset/</code> folder and CleanVision will work with <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html" rel="noopener noreferrer" target="_blank">most image formats</a>.
Here's a runnable <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://colab.research.google.com/github/cleanlab/cleanvision-examples/blob/main/caltech256.ipynb" rel="noopener noreferrer" target="_blank">notebook</a> with this code for you to try out. Running this CleanVision code on the Caltech-256 images produces the following <b>report</b>:
</p>

<div><img alt="Caltech-256 CleanVision Report part 1" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport1.png" style="color:transparent" width="1200"/></div>
<div><img alt="Caltech-256 CleanVision Report part 2" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport2.png" style="color:transparent" width="1200"/></div>
<div><img alt="Caltech-256 CleanVision Report part 3" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport3.png" style="color:transparent" width="1200"/></div>
<div><img alt="Caltech-256 CleanVision Report part 4" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport4.png" style="color:transparent" width="1200"/></div>
<div><img alt="Caltech-256 CleanVision Report part 5" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport5.png" style="color:transparent" width="1200"/></div>
<div><img alt="Caltech-256 CleanVision Report part 6" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport6.png" style="color:transparent" width="1200"/></div>
<div><img alt="Caltech-256 CleanVision Report part 7" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport7.png" style="color:transparent" width="1200"/></div>
<div><img alt="Caltech-256 CleanVision Report part 8" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport8.png" style="color:transparent" width="1200"/></div>
<div><img alt="Caltech-256 CleanVision Report part 9" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport9.png" style="color:transparent" width="1200"/></div>
<div><img alt="Caltech-256 CleanVision Report part 10" blurheight="6" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="825" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cleanvisionreport10.png" style="color:transparent" width="1200"/></div>



<h2 class="text-pretty" id="why-is-image-data-quality-important">Why is image data quality important for AI?</h2>
<p>Consider the following quotes from the <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://openai.com/index/dall-e-2-pre-training-mitigations/" rel="noopener noreferrer" target="_blank">OpenAI blog on DALLE-2</a>, one of the best generative image models available today produced by an organization that has created many of the world’s best ML models. While these quotes are specific to generative modeling, a wealth of literature on <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://dcai.csail.mit.edu" rel="noopener noreferrer" target="_blank">data-centric AI</a> has found these points highly relevant in other computer vision tasks.</p>
<blockquote class="border-l-3 type-body-300 mx-0 my-5 border-l-border-0 p-0 pl-8 text-text-primary">
<p>Since training data shapes the capabilities of any learned model, data filtering is a powerful tool for limiting undesirable model capabilities.</p>
</blockquote>
<blockquote class="border-l-3 type-body-300 mx-0 my-5 border-l-border-0 p-0 pl-8 text-text-primary">
<p>We prioritized filtering out all of the bad data over leaving in all of the good data. This is because we can always fine-tune our model with more data later to teach it new things, but it’s much harder to make the model forget something that it has already learned.</p>
</blockquote>
<p>We designed CleanVision to help you filter out the bad data in your applications, like OpenAI did before training DALLE-2.</p>
<blockquote class="border-l-3 type-body-300 mx-0 my-5 border-l-border-0 p-0 pl-8 text-text-primary">
<p>When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. […] Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.</p>
</blockquote>
<blockquote class="border-l-3 type-body-300 mx-0 my-5 border-l-border-0 p-0 pl-8 text-text-primary">
<p>To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. […] we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.</p>
</blockquote>
<p>These are merely a few of the reasons behind the need to check for issues like (near) duplicates and low information images.
Each type of issue that CleanVision detects is something that might hinder training the best possible model on your data.</p>
<h2 class="text-pretty" id="why-should-cleanvision-be-a-first-step-in-computer-vision-workflows">Why should CleanVision be a first step in computer vision workflows?</h2>
<p>Beyond being easy to use and fast to run, CleanVision offers more <strong class="[font-weight:600]">comprehensive coverage</strong> of possible issues in an image dataset. Such automated methods are much more <strong class="[font-weight:600]">systematic</strong> than manually checking the data – CleanVision results are <strong class="[font-weight:600]">reproducible</strong> and can be used to compare datasets directly.</p>
<p>Unlike most computer vision packages today which require GPUs, CleanVision can efficiently audit most image datasets on a CPU. Since it focuses on the image files themselves, CleanVision can be <strong class="[font-weight:600]">useful in all computer vision tasks</strong>, including supervised learning and generative modeling. For supervised learning datasets, CleanVision does not audit the quality of the dataset labels – instead use the <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://github.com/cleanlab/cleanlab" rel="noopener noreferrer" target="_blank">cleanlab</a> library to easily do that.</p>
<h2 class="text-pretty" id="issues-in-other-famous-image-datasets">Issues in other famous image datasets</h2>
<p>We used the exact same few lines of code shown above to run CleanVision on other famous image datasets cited in hundreds of papers each year: Food101, CUB-200-2011, and CIFAR-10. These datasets are large enough that manual identification of issues is impractical. Although these datasets are quite diverse, we only needed to run the exact same 3 lines of code shown above to audit each one.</p>
<p>For datasets with train and test splits, we merged these into a single dataset prior to running CleanVision. Below, we summarize some issues detected in these datasets. Notebooks to reproduce these results and see all of the detected issues are available in the <a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://github.com/cleanlab/cleanvision-examples" rel="noopener noreferrer" target="_blank">cleanvision-examples</a> repository.</p>
<h3 class="text-pretty" id="food101-dataset">Food101 Dataset</h3>
<div><img alt="Issues in the Food-101 dataset" blurheight="5" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="1020" loading="lazy" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/food101.png" style="color:transparent" width="1200"/></div>
<p><a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/" rel="noopener noreferrer" target="_blank">Food-101</a> consists of 101 food categories with
1000 images per category. CleanVision detected several issues in this dataset like images that are dark, blurry, exact duplicates, near duplicates and grayscale. In fact, it found a few grayscale images
which are not food images at all! Additionally, there are around 100 pairs of near and exact duplicates
in the dataset with a slight change in lighting or some text in the image. As previously discussed, uniqueness<sup class="top-[-0.85em] ml-[0.15em] text-[0.65em]"><a aria-describedby="footnote-label" class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" data-footnote-ref="true" href="https://cleanlab.ai/blog/learn/cleanvision/#user-content-fn-3" id="user-content-fnref-3">4</a></sup> is an important aspect of data quality and having (near) duplicate images in the dataset can negatively impact model performance. The last row depicted above consists of dark images detected by CleanVision, which are labeled as: <em class="italic">breakfast_burrito</em>, <em class="italic">bibimbap</em>, <em class="italic">macarons</em>, <em class="italic">bread_pudding</em>, <em class="italic">bread_pudding</em>, <em class="italic">oysters</em> in order from left to right. However, the correctness of these labels is hard to verify, and thus dark/blurry images can contribute to noise in the dataset.</p>
<h3 class="text-pretty" id="cub-200-2011-dataset">CUB-200-2011 Dataset</h3>
<div><img alt="Issues in the CUB-200-2011 dataset" blurheight="4" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="362" loading="lazy" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cubs2011.png" style="color:transparent" width="1200"/></div>
<p><a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://www.vision.caltech.edu/datasets/cub_200_2011/" rel="noopener noreferrer" target="_blank">Caltech-UCSD Birds-200-2011 (CUB-200-2011)</a> contains
11,788 images depicting birds from 200 subcategories. CleanVision detected fewer issues in this dataset compared
to Caltech-256 and Food-101, but there are still some noteworthy issues depicted above. Indicative of a labeling error, the pair of nearly-duplicated brown birds actually belong to 2 different classes – one is labeled as
<em class="italic">Whip Poor Will</em> and the other <em class="italic">Chuck Will Widow</em>.
Odd aspect ratio and blurry images can also be problematic, especially
for the task of fine-grained classification if important features of the bird are obfuscated. The low
information images in this dataset don’t look like real images but more like drawings and provide very
little information on the fine-grained features of the birds. The blue bird image flagged as low information has label <em class="italic">ivory gull</em>, which seems to be incorrect given that bird should be white.</p>
<p>The existence of exact duplicates in CUB-200-2011, Food-101, and Caltech-256 indicates that even basic curation steps were overlooked when establishing these ML benchmarks. The situation is more dire in many datasets from real-world applications, which are far less curated than these benchmark datasets (e.g. due to tight deadlines). The following table lists the number of near/exact duplicate images in each dataset we audited with CleanVision.</p>
<div class="overflow-hidden rounded-2 border border-border-2 has-[[data-scroll-area]:focus]:border-blue-700 has-[[data-scroll-area]:focus-visible]:ring has-[[data-scroll-area]:focus-visible]:ring-focus my-9"><div class="overflow-auto focus:outline-none focus-visible:outline-none" data-scroll-area="true"><table class="m-0 w-full border-collapse border-none p-0"><thead class="border-b border-border-0"><tr class="border-0 [&amp;:not(:last-child)_td]:border-b bg-surface-1 [&amp;:nth-child(even)]:bg-surface-1-hover"><th class="type-body-100-semibold border-0 px-4 py-3 text-start [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">Dataset</th><th class="type-body-100-semibold border-0 px-4 py-3 text-start [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0"># Near Duplicates</th><th class="type-body-100-semibold border-0 px-4 py-3 text-start [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0"># Exact Duplicates</th></tr></thead><tbody><tr class="border-0 [&amp;:not(:last-child)_td]:border-b bg-surface-0 [&amp;:nth-child(even)]:bg-surface-1"><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">Caltech-256</td><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">167</td><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">31</td></tr><tr class="border-0 [&amp;:not(:last-child)_td]:border-b bg-surface-0 [&amp;:nth-child(even)]:bg-surface-1"><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">Food-101</td><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">92</td><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">118</td></tr><tr class="border-0 [&amp;:not(:last-child)_td]:border-b bg-surface-0 [&amp;:nth-child(even)]:bg-surface-1"><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">CUB-200-2011</td><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">10</td><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">2</td></tr><tr class="border-0 [&amp;:not(:last-child)_td]:border-b bg-surface-0 [&amp;:nth-child(even)]:bg-surface-1"><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">CIFAR-10</td><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">40</td><td class="type-body-100 border-0 px-4 py-3 [&amp;:not(:last-child)]:border-r [&amp;:not(:last-child)]:border-r-border-0">0</td></tr></tbody></table></div></div>
<h3 class="text-pretty" id="cifar-10-dataset">CIFAR-10 Dataset</h3>
<div><img alt="Issues in the CIFAR-10 dataset" blurheight="4" blurwidth="8" class="max-h-80vh mx-auto my-8 block h-auto w-full max-w-full" data-nimg="1" decoding="async" height="367" loading="lazy" src="https://raw.githubusercontent.com/cleanlab/assets/master/cleanvision/blogfiles/cifar10.png" style="color:transparent" width="1200"/></div>
<p><a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener noreferrer" target="_blank">CIFAR-10</a> has 60,000 32x32 color images. This
dataset had the least number of issues amongst the ones we evaluated here. The most prevalent issues found in this dataset are near duplicated, dark, light, and low information images. The near duplicated car image is a form of <em class="italic">leakage
between train and test set</em> (the grayscale car image is in the train split whereas the same car image in color is part of the test split).
Such biased evaluation can give a false sense of the model’s accuracy and how well it will actually perform in deployment.
CleanVision finds that CIFAR-10 contains 20 sets of near duplicate images. There are also some very light/dark and low
information images in the dataset, from which it is very difficult to gather any useful features. It is hard to tell that
the light images are airplanes and the low information image is a bird. A model trained on such ill-posed data may learn spurious correlations if one is not careful.</p>
<h2 class="text-pretty" id="resources-to-learn-more">Resources to learn more</h2>
<ul>
<li><a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://github.com/cleanlab/cleanvision" rel="noopener noreferrer" target="_blank">GitHub Repo</a> - Check out the CleanVision source code.</li>
<li><a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://cleanvision.readthedocs.io/en/latest/tutorials/tutorial.html" rel="noopener noreferrer" target="_blank">Quickstart Tutorial</a> - See additional things you can do with CleanVision in 5min.</li>
<li><a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://github.com/cleanlab/cleanvision-examples" rel="noopener noreferrer" target="_blank">Example Notebooks</a> - Includes code to run CleanVision on the above datasets to reproduce all of the issues displayed in this article.</li>
<li><a class="relative -mx-px -mb-px cursor-pointer rounded-1 px-px pb-px focus-visible:ring-2 focus-visible:ring-focus max-w-fit text-text-strong underline hover:text-text-primary [&amp;:any-link]:focus-visible:outline-none" href="https://cleanvision.readthedocs.io/en/latest/" rel="noopener noreferrer" target="_blank">Documentation</a> - Understand the various APIs this package offers.</li>
</ul>
</section></div></div></div></div><script crossorigin="anonymous" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/surveys.js" type="text/javascript"></script><script crossorigin="anonymous" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/config.js" type="text/javascript"></script><script crossorigin="anonymous" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/dead-clicks-autocapture.js" type="text/javascript"></script><script crossorigin="anonymous" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/posthog-recorder.js" type="text/javascript"></script><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"cleanvision","data":{"title":"CleanVision: Audit your Image Data for better Computer Vision","authors":["sanjana","ulyana","yiming.chen","elias","jonas"],"date":"2023-03-22","thumbnail":"thumb.png","description":"Introducing an open-source Python package to automatically identify common issues in image datasets.","tags":["Product"]},"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    b: \"b\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    div: \"div\",\n    em: \"em\",\n    h2: \"h2\",\n    h3: \"h3\",\n    img: \"img\",\n    li: \"li\",\n    ol: \"ol\",\n    p: \"p\",\n    pre: \"pre\",\n    section: \"section\",\n    strong: \"strong\",\n    sup: \"sup\",\n    table: \"table\",\n    tbody: \"tbody\",\n    td: \"td\",\n    th: \"th\",\n    thead: \"thead\",\n    tr: \"tr\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  }, {Accordion} = _components;\n  if (!Accordion) _missingMdxReference(\"Accordion\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"Introducing \", _jsx(_components.a, {\n        href: \"https://github.com/cleanlab/cleanvision\",\n        children: \"CleanVision\"\n      }), \": an open-source Python library that scans \", _jsx(_components.strong, {\n        children: \"any image dataset\"\n      }), \" for common real-world \", _jsx(_components.strong, {\n        children: \"issues\"\n      }), \" such as images which are blurry, under/over-exposed, oddly sized, or (near) duplicates of others. Here are some of the issues CleanVision automatically detected (with just 3 lines of code) in the famous \", _jsx(_components.a, {\n        href: \"https://data.caltech.edu/records/nyy15-4j048\",\n        children: \"Caltech-256 dataset\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.div, {\n      children: _jsx(_components.img, {\n        src: \"caltech.png\",\n        width: \"1200\",\n        height: \"825\",\n        alt: \"!Issues detected in the Caltech-256 dataset\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Teaching computers \", _jsx(_components.em, {\n        children: \"to see\"\n      }), \" via massive datasets and models has produced astounding progress for image generation, classification, segmentation, and related tasks like object detection. As computer vision techniques graduate from the lab to real-world applications, data quality poses a major challenge\", _jsx(_components.sup, {\n        children: _jsx(_components.a, {\n          href: \"#user-content-fn-1\",\n          id: \"user-content-fnref-1\",\n          \"data-footnote-ref\": true,\n          \"aria-describedby\": \"footnote-label\",\n          children: \"1\"\n        })\n      }), \". ML models can only be as good as the data they are trained on.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"To help you improve your data, we’ve just open-sourced \", _jsx(_components.a, {\n        href: \"https://github.com/cleanlab/cleanvision\",\n        children: \"CleanVision\"\n      }), \", a package to quickly audit \", _jsx(_components.em, {\n        children: \"any\"\n      }), \" image dataset for a broad range of common issues lurking in real-world data.\\nInstead of relying on manual inspection, which can be time-consuming and lack coverage, CleanVision provides an automated systematic approach for detecting data issues.\\nWith just a few lines of code (that can be used to audit any image data), we ran CleanVision on some popular computer vision datasets and identified many issues in each.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"issues-in-the-caltech-256-dataset\",\n      children: \"Issues in the Caltech-256 dataset\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Widely used for benchmarking object recognition methods, the \", _jsx(_components.a, {\n        href: \"https://data.caltech.edu/records/nyy15-4j048\",\n        children: \"Caltech-256\"\n      }), \" dataset consists of 30,607 real-world images (of different sizes) stemming from 257 classes.\\nCleanVision identified several issues in this dataset including images that are: grayscale, low information, blurry, near duplicates, or have an odd aspect ratio. All of the images in the \", _jsx(_components.em, {\n        children: \"car-side\"\n      }), \" class are grayscale images, which can cause models to spuriously associate\", _jsx(_components.sup, {\n        children: _jsx(_components.a, {\n          href: \"#user-content-fn-4\",\n          id: \"user-content-fnref-4\",\n          \"data-footnote-ref\": true,\n          \"aria-describedby\": \"footnote-label\",\n          children: \"2\"\n        })\n      }), \" the \", _jsx(_components.em, {\n        children: \"car-side\"\n      }), \" class with the grayscale attribute rather than focusing on the object’s features. Moreover, CleanVision detected low-information images that do not resemble real-world images like stick figures and graphs. Closer inspection reveals some of these images are mislabeled in the dataset. For example, the sigmoid like plot in the low information set of images is wrongly labeled as a \", _jsx(_components.em, {\n        children: \"housefly\"\n      }), \". Detecting low-information images is also important when training generative models, as these can be easily memorized by\\nthe model and lead to model regurgitation\", _jsx(_components.sup, {\n        children: _jsx(_components.a, {\n          href: \"#user-content-fn-2\",\n          id: \"user-content-fnref-2\",\n          \"data-footnote-ref\": true,\n          \"aria-describedby\": \"footnote-label\",\n          children: \"3\"\n        })\n      }), \" instead of synthesizing new images.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"More generally, it is good to know these issues exist in your data before you dive into modeling it. Especially if they are easily detected with CleanVision!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Getting the above results for the Caltech-256 dataset was as easy as just running this code:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-python\",\n        children: \"from cleanvision.imagelab import Imagelab\\n\\nimagelab = Imagelab(data_path=\\\"path_to_dataset\\\")\\nimagelab.find_issues()\\nimagelab.report()\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"The image files can simply live in a \", _jsx(_components.code, {\n        children: \"path_to_dataset/\"\n      }), \" folder and CleanVision will work with \", _jsx(_components.a, {\n        href: \"https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html\",\n        children: \"most image formats\"\n      }), \".\"]\n    }), \"\\n\", _jsxs(Accordion, {\n      question: _jsxs(_Fragment, {\n        children: [\"For the Caltech-256 images, CleanVision outputs the following report (\", _jsx(_components.b, {\n          children: \"click to view it\"\n        }), \")\"]\n      }),\n      children: [_jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output1.png\",\n          width: \"1568\",\n          height: \"512\",\n          alt: \"CleanVision report for Caltech-256 - part 1\"\n        })\n      }), _jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output2.png\",\n          width: \"1560\",\n          height: \"526\",\n          alt: \"CleanVision report for Caltech-256 - part 2\"\n        })\n      }), _jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output3.png\",\n          width: \"1560\",\n          height: \"508\",\n          alt: \"CleanVision report for Caltech-256 - part 3\"\n        })\n      }), _jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output4.png\",\n          width: \"1608\",\n          height: \"468\",\n          alt: \"CleanVision report for Caltech-256 - part 4\"\n        })\n      }), _jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output5.png\",\n          width: \"1602\",\n          height: \"1460\",\n          alt: \"CleanVision report for Caltech-256 - part 5\"\n        })\n      }), _jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output6.png\",\n          width: \"1600\",\n          height: \"524\",\n          alt: \"CleanVision report for Caltech-256 - part 6\"\n        })\n      }), _jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output7.png\",\n          width: \"1622\",\n          height: \"468\",\n          alt: \"CleanVision report for Caltech-256 - part 7\"\n        })\n      }), _jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output8.png\",\n          width: \"1612\",\n          height: \"416\",\n          alt: \"CleanVision report for Caltech-256 - part 8\"\n        })\n      }), _jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output9.png\",\n          width: \"1614\",\n          height: \"1290\",\n          alt: \"CleanVision report for Caltech-256 - part 9\"\n        })\n      }), _jsx(_components.div, {\n        children: _jsx(_components.img, {\n          src: \"outputs/output10.png\",\n          width: \"1598\",\n          height: \"870\",\n          alt: \"CleanVision report for Caltech-256 - part 10\"\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can use this \", _jsx(_components.a, {\n        href: \"https://colab.research.google.com/github/cleanlab/cleanvision-examples/blob/main/caltech256.ipynb\",\n        children: \"notebook\"\n      }), \" to easily try it yourself!\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"why-is-image-data-quality-important\",\n      children: \"Why is image data quality important?\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Consider the following quotes from the OpenAI blog on DALLE-2\", _jsx(_components.sup, {\n        children: _jsx(_components.a, {\n          href: \"#user-content-fn-2\",\n          id: \"user-content-fnref-2-2\",\n          \"data-footnote-ref\": true,\n          \"aria-describedby\": \"footnote-label\",\n          children: \"3\"\n        })\n      }), \", one of the best generative image models available today produced by an organization that has created many of the world’s best ML models. While these quotes are specific to generative modeling, a wealth of literature on data-centric AI\", _jsx(_components.sup, {\n        children: _jsx(_components.a, {\n          href: \"#user-content-fn-1\",\n          id: \"user-content-fnref-1-2\",\n          \"data-footnote-ref\": true,\n          \"aria-describedby\": \"footnote-label\",\n          children: \"1\"\n        })\n      }), \" has found these points highly relevant in other computer vision tasks.\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"Since training data shapes the capabilities of any learned model, data filtering is a powerful tool for limiting undesirable model capabilities.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"We prioritized filtering out all of the bad data over leaving in all of the good data. This is because we can always fine-tune our model with more data later to teach it new things, but it’s much harder to make the model forget something that it has already learned.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We designed CleanVision to help you filter out the bad data in your applications, like OpenAI did before training DALLE-2.\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. […] Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. […] we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"These are merely a few of the reasons behind the need to check for issues like (near) duplicates and low information images.\\nEach type of issue that CleanVision detects is something that might hinder training the best possible model on your data.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"why-should-cleanvision-be-a-first-step-in-computer-vision-workflows\",\n      children: \"Why should CleanVision be a first step in computer vision workflows?\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Beyond being easy to use and fast to run, CleanVision offers more \", _jsx(_components.strong, {\n        children: \"comprehensive coverage\"\n      }), \" of possible issues in an image dataset. Such automated methods are much more \", _jsx(_components.strong, {\n        children: \"systematic\"\n      }), \" than manually checking the data – CleanVision results are \", _jsx(_components.strong, {\n        children: \"reproducible\"\n      }), \" and can be used to compare datasets directly.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Unlike most computer vision packages today which require GPUs, CleanVision can efficiently audit most image datasets on a CPU. Since it focuses on the image files themselves, CleanVision can be \", _jsx(_components.strong, {\n        children: \"useful in all computer vision tasks\"\n      }), \", including supervised learning and generative modeling. For supervised learning datasets, CleanVision does not audit the quality of the dataset labels – instead use the \", _jsx(_components.a, {\n        href: \"https://github.com/cleanlab/cleanlab\",\n        children: \"cleanlab\"\n      }), \" library to easily do that.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"issues-in-other-famous-image-datasets\",\n      children: \"Issues in other famous image datasets\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We used the exact same few lines of code shown above to run CleanVision on other famous image datasets cited in hundreds of papers each year: Food101, CUB-200-2011, and CIFAR-10. These datasets are large enough that manual identification of issues is impractical. Although these datasets are quite diverse, we only needed to run the exact same 3 lines of code shown above to audit each one.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"For datasets with train and test splits, we merged these into a single dataset prior to running CleanVision. Below, we summarize some issues detected in these datasets. Notebooks to reproduce these results and see all of the detected issues are available in the \", _jsx(_components.a, {\n        href: \"https://github.com/cleanlab/cleanvision-examples\",\n        children: \"cleanvision-examples\"\n      }), \" repository.\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"food101-dataset\",\n      children: \"Food101 Dataset\"\n    }), \"\\n\", _jsx(_components.div, {\n      children: _jsx(_components.img, {\n        src: \"food101.png\",\n        width: \"1501\",\n        height: \"1020\",\n        alt: \"Issues in the Food-101 dataset\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/\",\n        children: \"Food-101\"\n      }), \" consists of 101 food categories with\\n1000 images per category. CleanVision detected several issues in this dataset like images that are dark, blurry, exact duplicates, near duplicates and grayscale. In fact, it found a few grayscale images\\nwhich are not food images at all! Additionally, there are around 100 pairs of near and exact duplicates\\nin the dataset with a slight change in lighting or some text in the image. As previously discussed, uniqueness\", _jsx(_components.sup, {\n        children: _jsx(_components.a, {\n          href: \"#user-content-fn-3\",\n          id: \"user-content-fnref-3\",\n          \"data-footnote-ref\": true,\n          \"aria-describedby\": \"footnote-label\",\n          children: \"4\"\n        })\n      }), \" is an important aspect of data quality and having (near) duplicate images in the dataset can negatively impact model performance. The last row depicted above consists of dark images detected by CleanVision, which are labeled as: \", _jsx(_components.em, {\n        children: \"breakfast_burrito\"\n      }), \", \", _jsx(_components.em, {\n        children: \"bibimbap\"\n      }), \", \", _jsx(_components.em, {\n        children: \"macarons\"\n      }), \", \", _jsx(_components.em, {\n        children: \"bread_pudding\"\n      }), \", \", _jsx(_components.em, {\n        children: \"bread_pudding\"\n      }), \", \", _jsx(_components.em, {\n        children: \"oysters\"\n      }), \" in order from left to right. However, the correctness of these labels is hard to verify, and thus dark/blurry images can contribute to noise in the dataset.\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"cub-200-2011-dataset\",\n      children: \"CUB-200-2011 Dataset\"\n    }), \"\\n\", _jsx(_components.div, {\n      children: _jsx(_components.img, {\n        src: \"cubs2011.png\",\n        width: \"700\",\n        height: \"362\",\n        alt: \"Issues in the CUB-200-2011 dataset\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"https://www.vision.caltech.edu/datasets/cub_200_2011/\",\n        children: \"Caltech-UCSD Birds-200-2011 (CUB-200-2011)\"\n      }), \" contains\\n11,788 images depicting birds from 200 subcategories. CleanVision detected fewer issues in this dataset compared\\nto Caltech-256 and Food-101, but there are still some noteworthy issues depicted above. Indicative of a labeling error, the pair of nearly-duplicated brown birds actually belong to 2 different classes – one is labeled as\\n\", _jsx(_components.em, {\n        children: \"Whip Poor Will\"\n      }), \" and the other \", _jsx(_components.em, {\n        children: \"Chuck Will Widow\"\n      }), \".\\nOdd aspect ratio and blurry images can also be problematic, especially\\nfor the task of fine-grained classification if important features of the bird are obfuscated. The low\\ninformation images in this dataset don’t look like real images but more like drawings and provide very\\nlittle information on the fine-grained features of the birds. The blue bird image flagged as low information has label \", _jsx(_components.em, {\n        children: \"ivory gull\"\n      }), \", which seems to be incorrect given that bird should be white.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The existence of exact duplicates in CUB-200-2011, Food-101, and Caltech-256 indicates that even basic curation steps were overlooked when establishing these ML benchmarks. The situation is more dire in many datasets from real-world applications, which are far less curated than these benchmark datasets (e.g. due to tight deadlines). The following table lists the number of near/exact duplicate images in each dataset we audited with CleanVision.\"\n    }), \"\\n\", _jsxs(_components.table, {\n      children: [_jsx(_components.thead, {\n        children: _jsxs(_components.tr, {\n          children: [_jsx(_components.th, {\n            children: \"Dataset\"\n          }), _jsx(_components.th, {\n            children: \"# Near Duplicates\"\n          }), _jsx(_components.th, {\n            children: \"# Exact Duplicates\"\n          })]\n        })\n      }), _jsxs(_components.tbody, {\n        children: [_jsxs(_components.tr, {\n          children: [_jsx(_components.td, {\n            children: \"Caltech-256\"\n          }), _jsx(_components.td, {\n            children: \"167\"\n          }), _jsx(_components.td, {\n            children: \"31\"\n          })]\n        }), _jsxs(_components.tr, {\n          children: [_jsx(_components.td, {\n            children: \"Food-101\"\n          }), _jsx(_components.td, {\n            children: \"92\"\n          }), _jsx(_components.td, {\n            children: \"118\"\n          })]\n        }), _jsxs(_components.tr, {\n          children: [_jsx(_components.td, {\n            children: \"CUB-200-2011\"\n          }), _jsx(_components.td, {\n            children: \"10\"\n          }), _jsx(_components.td, {\n            children: \"2\"\n          })]\n        }), _jsxs(_components.tr, {\n          children: [_jsx(_components.td, {\n            children: \"CIFAR-10\"\n          }), _jsx(_components.td, {\n            children: \"40\"\n          }), _jsx(_components.td, {\n            children: \"0\"\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"cifar-10-dataset\",\n      children: \"CIFAR-10 Dataset\"\n    }), \"\\n\", _jsx(_components.div, {\n      children: _jsx(_components.img, {\n        src: \"cifar10.png\",\n        width: \"700\",\n        height: \"367\",\n        alt: \"Issues in the CIFAR-10 dataset\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"https://www.cs.toronto.edu/~kriz/cifar.html\",\n        children: \"CIFAR-10\"\n      }), \" has 60,000 32x32 color images. This\\ndataset had the least number of issues amongst the ones we evaluated here. The most prevalent issues found in this dataset are near duplicated, dark, light, and low information images. The near duplicated car image is a form of \", _jsx(_components.em, {\n        children: \"leakage\\nbetween train and test set\"\n      }), \" (the grayscale car image is in the train split whereas the same car image in color is part of the test split).\\nSuch biased evaluation can give a false sense of the model’s accuracy and how well it will actually perform in deployment.\\nCleanVision finds that CIFAR-10 contains 20 sets of near duplicate images. There are also some very light/dark and low\\ninformation images in the dataset, from which it is very difficult to gather any useful features. It is hard to tell that\\nthe light images are airplanes and the low information image is a bird. A model trained on such ill-posed data may learn spurious correlations\", _jsx(_components.sup, {\n        children: _jsx(_components.a, {\n          href: \"#user-content-fn-4\",\n          id: \"user-content-fnref-4-2\",\n          \"data-footnote-ref\": true,\n          \"aria-describedby\": \"footnote-label\",\n          children: \"2\"\n        })\n      }), \" if we are not careful.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"resources-to-learn-more\",\n      children: \"Resources to learn more\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"https://github.com/cleanlab/cleanvision\",\n          children: \"GitHub Repo\"\n        }), \" - Check out the CleanVision code and contribute your own ideas.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"https://cleanvision.readthedocs.io/en/latest/tutorials/tutorial.html\",\n          children: \"Quickstart Tutorial\"\n        }), \" - See additional things you can do with CleanVision in 5min.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"https://github.com/cleanlab/cleanvision-examples\",\n          children: \"Example Notebooks\"\n        }), \" - Includes code to run CleanVision on the above datasets to reproduce all of the issues displayed in this article.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"https://cleanvision.readthedocs.io/en/latest/\",\n          children: \"Documentation\"\n        }), \" - Understand the various APIs this package offers.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"/slack/\",\n          children: \"Slack Community\"\n        }), \" - Join our community of scientists/engineers to ask questions, see how others are using CleanVision, and brainstorm new ideas.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"/\",\n          children: \"Cleanlab Studio\"\n        }), \" - A no-code solution to automatically fix data and label issues in image datasets (instantly apply all of the functionality in CleanVision + much more on big datasets).\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This is only the start. We have a much larger “vision” for the CleanVision package, and would love your help inventing the future of open-source Data-Centric AI!\"\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"references\",\n      children: \"References\"\n    }), \"\\n\", _jsxs(_components.section, {\n      \"data-footnotes\": true,\n      className: \"footnotes\",\n      children: [_jsx(_components.h2, {\n        className: \"sr-only\",\n        id: \"footnote-label\",\n        children: \"Footnotes\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          id: \"user-content-fn-1\",\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.a, {\n              href: \"https://spectrum.ieee.org/andrew-ng-data-centric-ai\",\n              children: \"“Andrew Ng: Unbiggen AI”\"\n            }), \" \", _jsx(_components.a, {\n              href: \"#user-content-fnref-1\",\n              \"data-footnote-backref\": \"\",\n              \"aria-label\": \"Back to reference 1\",\n              className: \"data-footnote-backref\",\n              children: \"↩\"\n            }), \" \", _jsxs(_components.a, {\n              href: \"#user-content-fnref-1-2\",\n              \"data-footnote-backref\": \"\",\n              \"aria-label\": \"Back to reference 1-2\",\n              className: \"data-footnote-backref\",\n              children: [\"↩\", _jsx(_components.sup, {\n                children: \"2\"\n              })]\n            })]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          id: \"user-content-fn-4\",\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.a, {\n              href: \"https://www.nature.com/articles/s41467-019-08987-4\",\n              children: \"“Unmasking Clever Hans predictors and assessing what machines really learn”\"\n            }), \" \", _jsx(_components.a, {\n              href: \"#user-content-fnref-4\",\n              \"data-footnote-backref\": \"\",\n              \"aria-label\": \"Back to reference 2\",\n              className: \"data-footnote-backref\",\n              children: \"↩\"\n            }), \" \", _jsxs(_components.a, {\n              href: \"#user-content-fnref-4-2\",\n              \"data-footnote-backref\": \"\",\n              \"aria-label\": \"Back to reference 2-2\",\n              className: \"data-footnote-backref\",\n              children: [\"↩\", _jsx(_components.sup, {\n                children: \"2\"\n              })]\n            })]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          id: \"user-content-fn-2\",\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.a, {\n              href: \"https://openai.com/index/dall-e-2-pre-training-mitigations/\",\n              children: \"“DALL.E 2 pre-training mitigations”\"\n            }), \" \", _jsx(_components.a, {\n              href: \"#user-content-fnref-2\",\n              \"data-footnote-backref\": \"\",\n              \"aria-label\": \"Back to reference 3\",\n              className: \"data-footnote-backref\",\n              children: \"↩\"\n            }), \" \", _jsxs(_components.a, {\n              href: \"#user-content-fnref-2-2\",\n              \"data-footnote-backref\": \"\",\n              \"aria-label\": \"Back to reference 3-2\",\n              className: \"data-footnote-backref\",\n              children: [\"↩\", _jsx(_components.sup, {\n                children: \"2\"\n              })]\n            })]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          id: \"user-content-fn-3\",\n          children: [\"\\n\", _jsxs(_components.p, {\n            children: [_jsx(_components.a, {\n              href: \"https://web.archive.org/web/20250613200432/https:%2F%2Fai-infrastructure.org/2847-2/\",\n              children: \"“How to Prioritize Data Quality for Computer Vision: An Expert Primer”\"\n            }), \" \", _jsx(_components.a, {\n              href: \"#user-content-fnref-3\",\n              \"data-footnote-backref\": \"\",\n              \"aria-label\": \"Back to reference 4\",\n              className: \"data-footnote-backref\",\n              children: \"↩\"\n            })]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}},"relatedBlogs":[{"slug":"safeguarding_personal_data_with_tlm","data":{"title":"Safeguard Customer Data via Log Compliance Monitoring with the Trustworthy Language Model","authors":["matt.turk"],"date":"2025-01-06","thumbnail":{"path":"thumb.jpg","width":322,"height":157},"socialPreviewCard":"thumb.jpg","description":"How enterprises can use LLMs to reliably catch compliance violations like GDPR from log files.","tags":["Product"]}},{"slug":"agent-tlm-hallucination-benchmarking","data":{"title":"Benchmarking real-time trust scoring across five AI Agent architectures","authors":["gordon","jonas"],"date":"2025-08-20","thumbnail":{"path":"thumb.png","width":1200,"height":630},"description":"Using AgentLite to study how much LLM trust scoring can reduce incorrect responses from popular agentic frameworks: Act, ReAct (zero/few shot), PlanAct, PlanReAct.","tags":["Product","RAG","AI Agents"],"pin":true}},{"slug":"llm-accuracy","data":{"title":"Automatically boost the accuracy of any LLM, without changing your prompts or the model","authors":["huiwen","jay_zhang","ulyana","jonas"],"date":"2024-10-31","thumbnail":{"path":"thumbnail.png","width":1189,"height":790},"description":"Demonstrating how the Trustworthy Language Model system can produce better responses from a wide variety of LLMs","tags":["Product","RAG"]}}],"nextBlog":"transformer-sklearn","otherBlogs":[{"slug":"series-a-announcement","data":{"title":"Letter from the CEO: Announcing our Series A and Cleanlab's Trustworthy Language Model","authors":["curtis"],"date":"2023-10-10","thumbnail":{"path":"founders.jpg","width":1000,"height":660},"description":"A personal perspective on the importance of clean data as Cleanlab announces $30M in funding to bring automated data curation to enterprise AI.","tags":["News","Product"],"previous":"/blog/announcing-cleanlab-studio/","next":"/blog/data-centric-ai/"}},{"slug":"reliable-agentic-rag","data":{"title":"Reliable Agentic RAG with LLM Trustworthiness Estimates","authors":["chris","jonas"],"date":"2024-09-12","thumbnail":{"path":"RAG_diagram.png","width":1560,"height":780},"description":"Ensure reliable answers in Retrieval-Augmented Generation, while also ensuring that latency and compute costs do not exceed the processing needed to accurately respond to complex queries.","tags":["RAG","Product","AI Agents"]}},{"slug":"ai-agent-safety","data":{"title":"AI Agent Safety: Managing Unpredictability at Scale","authors":["dave"],"date":"2025-09-08","thumbnail":{"path":"images/thumbnail_ai-agent-safety.png","width":1200,"height":675},"socialPreviewCard":"images/social-share_agent-risk.png","description":"AI agents are moving into enterprise workflows, but unpredictability remains at every step. Leaders must understand four risk surfaces and how to contain them with layered safety systems.","order":5,"tags":["AI Agents"],"pin":true}},{"slug":"prevent-hallucinated-responses","data":{"title":"Prevent Hallucinated Responses from any AI Agent","authors":["gordon","dave"],"date":"2025-06-30","thumbnail":{"path":"thumbnail.png","width":1201,"height":673},"socialPreviewCard":"articleFigure.png","description":"A case study on a reliable Customer Support Agent built with LangGraph and automated trustworthiness scoring","tags":["Product","RAG","AI Agents"],"pin":true}},{"slug":"expert-guidance","data":{"title":"Expert Guidance: Teaching Your AI How to Behave","authors":["jonas","ulyana","anish","dave","charles"],"date":"2025-11-18","thumbnail":{"path":"images/thumbnail_expert-guidance.png","width":1200,"height":673},"socialPreviewCard":"images/social-share_expert-guidance.png","description":"Once your AI agents are live, the hard part begins: keeping them reliable. Cleanlab’s new Expert Guidance feature shows how non-engineers can teach AI systems to think and act better instantly, in natural language.","tags":["AI Agents","Product"],"pin":true,"featured":true}},{"slug":"tlm-structured-outputs-benchmark","data":{"title":"Real-Time Error Detection for LLM Structured Outputs:  A Comprehensive Benchmark","authors":["huiwen","jonas"],"date":"2025-12-12","thumbnail":{"path":"thumb.png","width":1200,"height":666},"description":"Tools to automatically detect errors in Structured Outputs or Extracted Data produced by any LLM.","tags":["Research","Product"],"pin":true,"featured":true}},{"slug":"rag-evaluation-models","data":{"title":"Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?","authors":["ashish","jonas"],"date":"2025-04-07","thumbnail":{"path":"thumbnail.png","width":716,"height":437},"description":"A comprehensive benchmark of evaluation models to automatically catch incorrect responses across five RAG applications.","tags":["Research","RAG","Product"],"pin":true}},{"slug":"4o-claude","data":{"title":"Automatically detecting LLM hallucinations with models like GPT-4o and Claude","authors":["huiwen","jay_zhang","ulyana","jonas"],"date":"2024-09-04","thumbnail":{"path":"thumbnail.png","width":872,"height":490},"description":"Benchmarking hallucination detection via the Trustworthy Language Model, with the newest models from OpenAI and Anthropic.","tags":["Product","RAG"]}},{"slug":"simpleqa","data":{"title":"Automatically Reduce Incorrect LLM Responses across OpenAI's SimpleQA Benchmark via Trustworthiness Scoring","authors":["huiwen","jonas"],"date":"2024-11-07","thumbnail":{"path":"gpt4o-benchmark.png","width":1010,"height":586},"description":"Benchmarking LLM trustworthiness scoring mechanisms to improve LLM abstention and response-generation.","tags":["Research","Product"]}},{"slug":"inside-trustworthiness-guardrail","data":{"title":"Preventing AI Mistakes in Production: Inside Cleanlab’s Guardrails","authors":["charles","dave"],"date":"2025-10-30","thumbnail":{"path":"images/inside-trustworthiness-guardrail_thumbnail.png","width":1200,"height":675},"socialPreviewCard":"images/inside-trustworthiness-guardrail_social-share.png","description":"Even advanced AI models still hallucinate, producing confident but wrong answers that can harm trust and compliance. Cleanlab’s trustworthiness guardrails, powered by the Trustworthy Language Model (TLM), block inaccurate responses in real time and deliver safe fallback or expert-verified answers to keep AI systems reliable in production.","tags":["AI Agents","Product"]}},{"slug":"trustworthy-language-model","data":{"title":"Overcoming Hallucinations with the Trustworthy Language Model","authors":["anish","jonas","curtis","huiwen","ulyana"],"date":"2024-04-25","thumbnail":{"path":"thumbnail.png","width":1200,"height":675},"description":"TLM scores the trustworthiness of outputs from any LLM in real-time via state-of-the-art uncertainty estimation.","tags":["Product","RAG"],"pin":true}},{"slug":"tlm-o1","data":{"title":"OpenAI's o1 surpassed using the Trustworthy Language Model","authors":["jay_zhang","jonas"],"date":"2024-10-21","thumbnail":{"path":"thumbnail.png","width":989,"height":590},"description":"See results from using the Trustworthy Language Model to: detect hallucinations/errors from the o1 model and improve its response accuracy.","tags":["Product"],"pin":true}},{"slug":"expert-answers","data":{"title":"Expert Answers: The Easiest Way to Improve Your AI Agent","authors":["dave","aditya"],"date":"2025-09-24","thumbnail":{"path":"images/thumbnail_expert-answers.png","width":1200,"height":675},"socialPreviewCard":"images/social-share_expert-answers.png","description":"AI agents often give wrong, IDK, or unhelpful answers that frustrate users. Expert Answers let nontechnical SMEs instantly fix these cases, making your AI more helpful without waiting for engineers.","order":5,"tags":["AI Agents","Product"],"pin":true,"featured":true}},{"slug":"structured-output-benchmark","data":{"title":"LLM Structured Output Benchmarks are Riddled with Mistakes","authors":["huiwen","jonas"],"date":"2025-12-05","thumbnail":{"path":"thumb.png","width":1200,"height":629},"description":"Existing Structured Outputs datasets are unreliable, so we created four new ones.","tags":["Research"],"pin":true}},{"slug":"emerging-reliability-layer-agent-stack","data":{"title":"The Emerging Reliability Layer in the Modern AI Agent Stack","authors":["charles"],"date":"2025-10-15","thumbnail":{"path":"images/thumbnail_reliability-layer.png","width":1200,"height":675},"socialPreviewCard":"images/social-share_reliability-layer.png","description":"AI agents succeed when teams separate the Core and Reliability stacks. The Core drives differentiation through architectures, prompts, tools, and context. Reliability ensures trust with guardrails, monitoring, and validations. Learn why the split matters and how top teams deliver agents that are both innovative and dependable.","order":5,"tags":["AI Agents"],"pin":true,"featured":true}},{"slug":"safeguarding_personal_data_with_tlm","data":{"title":"Safeguard Customer Data via Log Compliance Monitoring with the Trustworthy Language Model","authors":["matt.turk"],"date":"2025-01-06","thumbnail":{"path":"thumb.jpg","width":322,"height":157},"socialPreviewCard":"thumb.jpg","description":"How enterprises can use LLMs to reliably catch compliance violations like GDPR from log files.","tags":["Product"]}},{"slug":"rag-tlm-hallucination-benchmarking","data":{"title":"Benchmarking Hallucination Detection Methods in RAG","authors":["huiwen","nelson_auner","aditya","jonas"],"date":"2024-09-30","thumbnail":{"path":"rag-diagram.png","width":1024,"height":576},"description":"Evaluating state-of-the-art tools to automatically catch incorrect responses from a RAG system.","tags":["Research","RAG","Product"],"pin":true}},{"slug":"tau-bench","data":{"title":"Automated Hallucination Correction for AI Agents: A Case Study on Tau²-Bench","authors":["tianyi","jonas"],"date":"2025-12-03","thumbnail":{"path":"preview.png","width":1200,"height":634},"description":"Evaluating autonomous failure prevention for AI agents on the leading customer service AI benchmark.","tags":["TLM","AI Agents","New Research"],"pin":true,"featured":true}},{"slug":"llm-accuracy","data":{"title":"Automatically boost the accuracy of any LLM, without changing your prompts or the model","authors":["huiwen","jay_zhang","ulyana","jonas"],"date":"2024-10-31","thumbnail":{"path":"thumbnail.png","width":1189,"height":790},"description":"Demonstrating how the Trustworthy Language Model system can produce better responses from a wide variety of LLMs","tags":["Product","RAG"]}},{"slug":"managing-ai-apps-with-humans","data":{"title":"Managing AI Agents in Production: The Role of People","authors":["dave"],"date":"2025-09-23","thumbnail":{"path":"images/thumbnail_managing-ai-with-people.png","width":1200,"height":675},"socialPreviewCard":"images/social-share_managing-ai-with-people.png","description":"From guardrails to remediation, people keep AI agents aligned in production. Discover the oversight roles, levels of involvement, and steps engineering leaders can take to scale responsibly.","order":1,"tags":["AI Agents"],"pin":true}},{"slug":"agent-tlm-hallucination-benchmarking","data":{"title":"Benchmarking real-time trust scoring across five AI Agent architectures","authors":["gordon","jonas"],"date":"2025-08-20","thumbnail":{"path":"thumb.png","width":1200,"height":630},"description":"Using AgentLite to study how much LLM trust scoring can reduce incorrect responses from popular agentic frameworks: Act, ReAct (zero/few shot), PlanAct, PlanReAct.","tags":["Product","RAG","AI Agents"],"pin":true}},{"slug":"announcing-document-curation","data":{"title":"Don’t Let Your Messy Documents Run You RAG-Ged. Announcing Document Curation in Cleanlab Studio","authors":["emily"],"date":"2024-06-07","thumbnail":{"path":"thumbnail.png","width":1744,"height":980},"socialPreviewCard":"social-preview.png","metaTitle":"Curate large scale document collections - Cleanlab","description":"Generate AI, not headaches. Automate heterogenous data source curation with Cleanlab document support.","tags":["Product"]}},{"slug":"tlm-lite","data":{"title":"TLM Lite: High-Quality LLM Responses with Efficient Trust Scores","authors":["huiwen"],"date":"2024-09-09","thumbnail":{"path":"thumb.png","width":1600,"height":900},"description":"TLM Lite allows you to generate high-quality responses using advanced LLMs while employing smaller models for fast and cost-effective trustworthiness scoring.","tags":["Product"]}}]},"__N_SSG":true},"page":"/blog/learn/[slug]","query":{"slug":"cleanvision"},"buildId":"JV3ZnCMdPEiU-pIizhsMc","isFallback":false,"gsp":true,"scriptLoader":[]}</script><script type="text/javascript">_linkedin_partner_id = "6017994";
window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
window._linkedin_data_partner_ids.push(_linkedin_partner_id);</script><script type="text/javascript">(function(l) {
if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
window.lintrk.q=[]}
var s = document.getElementsByTagName("script")[0];
var b = document.createElement("script");
b.type = "text/javascript";b.async = true;
b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
s.parentNode.insertBefore(b, s);})(window.lintrk);</script><noscript><img alt="" height="1" src="https://px.ads.linkedin.com/collect/?pid=6017994&amp;fmt=gif" style="display:none" width="1"/></noscript><next-route-announcer><p aria-live="assertive" id="__next-route-announcer__" role="alert" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; top: 0px; width: 1px; white-space: nowrap; overflow-wrap: normal;"></p></next-route-announcer><iframe owner="archetype" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/saved_resource.html" style="display: none; visibility: hidden;" title="archetype"></iframe>
<div class="go2933276541 go2369186930" id="hs-web-interactives-top-anchor"><div class="go1632949049" id="hs-interactives-modal-overlay"></div></div>
<div class="go2933276541 go1348078617" id="hs-web-interactives-bottom-anchor"></div>
<div id="hs-web-interactives-floating-container">
<div class="go2417249464 go613305155" id="hs-web-interactives-floating-top-left-anchor">
</div>
<div class="go2417249464 go471583506" id="hs-web-interactives-floating-top-right-anchor">
</div>
<div class="go2417249464 go3921366393" id="hs-web-interactives-floating-bottom-left-anchor">
</div>
<div class="go2417249464 go3967842156" id="hs-web-interactives-floating-bottom-right-anchor">
</div>
</div>
<script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/8bdd1454-90018e4d60423a3c.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/5388-f1a9e1e47a423be7.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/9434-34c412f0c619f7d7.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/index-76410b38ed8a1cb4.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/contact-497081860c994020.js"></script><script async="" src="./CleanVision_ Audit your Image Data for better Computer Vision_files/zi-tag.js" type="text/javascript"></script><script async="" charset="utf-8" src="blob:https://cleanlab.ai/d2288915-6d63-4f0f-b5d9-246eb37124d8" type="text/javascript"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/9868-1b846e08fbe45822.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/2072-139672ef54e5e835.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/1100-affd1a526667f5f8.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/blog-61f654b447a5c73a.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/[slug]-19ab5eb3b9ed3d5c.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/detect-e09a8a05b427307a.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/about-b63f0a2bfa591698.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/2979-44437107383ef639.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/community-d47d6fe736b26899.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/remediate-0228767999a856e2.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/research-172a508a1c35af2d.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/csa-9f756dad5c5e06af.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/customers-6109daa8625247a8.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/partners-c0614dd8ecfcde4e.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/talks-5d64899916c05feb.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/team-fb11b7759cbf9e1f.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/ai-agents-in-production-2025-c28d00b1f2b644bf.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/careers-0667be188945c46a.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/news-70342c49dccc8ac5.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/trust-d4d7249583768725.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/terms-56f43b4a0e4f0f52.js"></script><script src="./CleanVision_ Audit your Image Data for better Computer Vision_files/privacy-9d4d93c42c7e0611.js"></script></main></body></html>